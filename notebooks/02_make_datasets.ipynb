{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580f9b75-fef1-4d60-9b3d-11e361f909f5",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Takes a list of site_codes and returns Daily Values og Gauge Height and Stream Flow from December 1 2015 to December 1 2020. The API url was formatted using the USGS API FWIS Formatting tool. https://waterservices.usgs.gov/rest/IV-Test-Tool.html.\n",
    "\n",
    "Individual API requests are made for each site in this version of the request. This requires us to do some indexing , and merging, in order to construct a datframe from our JSON formatted response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ecac63-01e5-4939-bb0c-43e336f4a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = [\"https://waterservices.usgs.gov/nwis/iv/?format=json&sites=\",\"&startDT=2015-12-01T00:00-0600&endDT=2020-12-01T00:00-0600&parameterCd=00060,00065&siteStatus=all\"]\n",
    "site_codes = ['07079300','07081200','07083710','07083200','07083000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4edf67f0-a5d3-4342-9c62-61ede124ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_height_data_pull(site_codes, file_path):\n",
    "\n",
    "    '''\n",
    "    file_path: string, target export directory. ex.) \"directory/\" \n",
    "    site_codes: list of strings\n",
    "\n",
    "    '''\n",
    "    url = [\"https://waterservices.usgs.gov/nwis/iv/?format=json&sites=\",\"&startDT=2015-12-01T00:00-0600&endDT=2020-12-01T00:00-0600&parameterCd=00060,00065&siteStatus=all\"]\n",
    "    df = pd.DataFrame()\n",
    "    for site in site_codes:\n",
    "\n",
    "        # API request\n",
    "        new_url = url[0]+site+url[1]\n",
    "        res = requests.get(new_url).json()\n",
    "\n",
    "        # indexing values & make df\n",
    "        data = res['value']['timeSeries']\n",
    "\n",
    "        StreamFlowValues = pd.DataFrame(data[0]['values'][0]['value']) \n",
    "        GaugeHeightValues = pd.DataFrame(data[1]['values'][0]['value']) \n",
    "\n",
    "        # drop extra and rename columns\n",
    "        try\n",
    "            StreamFlowValues.drop(columns = ['qualifiers'], inplace = True)\n",
    "        except: \n",
    "            print(site, ' No column qualifiers found, check for missing data.')\n",
    "            pass\n",
    "\n",
    "\n",
    "        try:\n",
    "            GaugeHeightValues.drop(columns = ['qualifiers'], inplace = True)\n",
    "        except: \n",
    "            print(site, ' No column qualifiers found, check for missing data.')\n",
    "            pass\n",
    "\n",
    "\n",
    "        try:\n",
    "            StreamFlowValues.columns =  ['flow','date']\n",
    "        except: \n",
    "            print(site, ' Could not rename column, check for missing data.')\n",
    "            pass\n",
    "\n",
    "\n",
    "        try:\n",
    "            GaugeHeightValues.columns = ['height','date']\n",
    "        except: \n",
    "            print(site, ' Could not rename column, check for missing data.')\n",
    "            pass\n",
    "\n",
    "\n",
    "        try:\n",
    "            temp_df = StreamFlowValues.merge(GaugeHeightValues, left_on='date',right_on='date')\n",
    "        except:\n",
    "            print(site, ' MISSING DATA ' ) \n",
    "            temp_df = StreamFlowValues\n",
    "\n",
    "        # create column for site_code \n",
    "        temp_df['sitecode'] = site\n",
    "\n",
    "        # output progress updates\n",
    "        print(f'Exporting {len(temp_df)} observations from site {site}')    \n",
    "        temp_df.to_csv(f'{file_path}{site}_height_flow_2015-20.csv')\n",
    "        df = pd.concat([df, temp_df])\n",
    "\n",
    "    print(f'\\nExporting {len(df)} total observations')    \n",
    "    df.to_csv(f'{file_path}arkansas_headwaters_height_flow_2015-20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab5e8a08-6ec1-42e6-b487-cd5e231b8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 5071 observations from site 07079300\n",
      "Exporting 543 observations from site 07081200\n",
      "Exporting 3 observations from site 07083710\n",
      "07083200  No column qualifiers found, check for missing data.\n",
      "07083200  Could not rename column, check for missing data.\n",
      "07083200  MISSING DATA \n",
      "Exporting 71994 observations from site 07083200\n",
      "Exporting 911 observations from site 07083000\n",
      "\n",
      "Exporting 78522 total observations\n"
     ]
    }
   ],
   "source": [
    "flow_height_data_pull(site_codes, 'datasets/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd372ec5-06a5-485d-b5e1-68b2a2fa73f9",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Next time we can :\n",
    "  1. call all of our sites in one API request at the same time.\n",
    "  2. wrap functions\n",
    "      * indexing \n",
    "      * cleaning\n",
    "      * merging\n",
    "      * naming\n",
    "  3. Create inputs with defauts to expand the API req:\n",
    "      * date ranges\n",
    "      * parameter codes\n",
    "      * response format \n",
    "      * site status\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
