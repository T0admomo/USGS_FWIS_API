{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580f9b75-fef1-4d60-9b3d-11e361f909f5",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Takes a list of site_codes and returns Daily Values og Gauge Height and Stream Flow from December 1 2015 to December 1 2020. The API url was formatted using the USGS API FWIS Formatting tool. https://waterservices.usgs.gov/rest/IV-Test-Tool.html.\n",
    "\n",
    "Individual API requests are made for each site in this version of the request. This requires us to do some indexing , and merging, in order to construct a datframe from our JSON formatted response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ecac63-01e5-4939-bb0c-43e336f4a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = [\"https://waterservices.usgs.gov/nwis/iv/?format=json&sites=\",\"&startDT=2015-12-01T00:00-0600&endDT=2020-12-01T00:00-0600&parameterCd=00060,00065&siteStatus=all\"]\n",
    "site_codes = ['07079300','07081200','07083710','07083200','07083000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4b1f40a-698a-4072-8be0-a7e463ae1a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 5071 observations from site 07079300\n",
      "Exporting 543 observations from site 07081200\n",
      "Exporting 3 observations from site 07083710\n",
      "07083200  No column qualifiers found, check for missing data.\n",
      "07083200  Could not rename column, check for missing data.\n",
      "07083200  MISSING DATA \n",
      "Exporting 71994 observations from site 07083200\n",
      "Exporting 911 observations from site 07083000\n",
      "Exporting 78522 total observations\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for site in site_codes:\n",
    "    \n",
    "    # API request\n",
    "    new_url = url[0]+site+url[1]\n",
    "    res = requests.get(new_url).json()\n",
    "    \n",
    "    # indexing values & make df\n",
    "    data = res['value']['timeSeries']\n",
    "    \n",
    "    StreamFlowValues = pd.DataFrame(data[0]['values'][0]['value']) \n",
    "    GaugeHeightValues = pd.DataFrame(data[1]['values'][0]['value']) \n",
    "\n",
    "    # drop extra and rename columns\n",
    "    try:\n",
    "        StreamFlowValues.drop(columns = ['qualifiers'], inplace = True)\n",
    "    except: \n",
    "        print(site, ' No column qualifiers found, check for missing data.')\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        GaugeHeightValues.drop(columns = ['qualifiers'], inplace = True)\n",
    "    except: \n",
    "        print(site, ' No column qualifiers found, check for missing data.')\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        StreamFlowValues.columns =  ['flow','date']\n",
    "    except: \n",
    "        print(site, ' Could not rename column, check for missing data.')\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        GaugeHeightValues.columns = ['height','date']\n",
    "    except: \n",
    "        print(site, ' Could not rename column, check for missing data.')\n",
    "        pass\n",
    "\n",
    "    \n",
    "    try:\n",
    "        temp_df = StreamFlowValues.merge(GaugeHeightValues, left_on='date',right_on='date')\n",
    "    except:\n",
    "        print(site, ' MISSING DATA ' ) \n",
    "        temp_df = StreamFlowValues\n",
    "\n",
    "    # create column for site_code \n",
    "    temp_df['sitecode'] = site\n",
    "    \n",
    "    # output progress updates\n",
    "    print(f'Exporting {len(temp_df)} observations from site {site}')    \n",
    "    temp_df.to_csv(f'datasets/{site}_height_flow_2015-20.csv')\n",
    "    df = pd.concat([df, temp_df])\n",
    "    \n",
    "print(f'Exporting {len(df)} total observations')    \n",
    "df.to_csv(f'datasets/arkansas_headwaters_height_flow_2015-20.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd372ec5-06a5-485d-b5e1-68b2a2fa73f9",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Next time we can :\n",
    "  1. call all of our sites in one API request at the same time.\n",
    "  2. wrap functions\n",
    "      * indexing, \n",
    "      * cleaning, \n",
    "      * merging\n",
    "      * naming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b96009-81b3-4143-9cf6-97b9e7603c2a",
   "metadata": {},
   "source": [
    "Once you have a dataset with all of the sites observations, and their respective site_codes, we can group by site_codes and compare their aggragetes. For now let's just take a look at one of our sites, and prep the data as timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f2474-5b4c-42cd-a0ae-694b0a52cfd1",
   "metadata": {},
   "source": [
    "___\n",
    "# Date Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2fd0191-ee42-445c-b83f-d68c5b70f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df = pd.read_csv('datasets/07083710_flow_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5f1e3-269b-44f1-8234-e7048fa8f122",
   "metadata": {},
   "source": [
    "https://newbedev.com/how-do-i-translate-an-iso-8601-datetime-string-into-a-python-datetime-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f2a51a1-3102-4bd9-8960-1dc8c69c1eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 10, 6, 11, 30, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=64800)))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "timestamp = df['date'][0]\n",
    "# This regex removes all colons and all\n",
    "# dashes EXCEPT for the dash indicating + or - utc offset for the timezone\n",
    "conformed_timestamp = re.sub(r\"[:]|([-](?!((\\d{2}[:]\\d{2})|(\\d{4}))$))\", '', timestamp)\n",
    "datetime.strptime(conformed_timestamp, \"%Y%m%dT%H%M%S.%f%z\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d60309-5313-4cf0-a20c-5eaeab8e0d6c",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=UFuo7EHI8zc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c8561bfa-d747-4f95-b9c2-61150815b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flow      float64\n",
       "date       object\n",
       "height    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7cbbe50-ceef-4f71-aae5-5fa7352afa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = ['']\n",
    "string = '2020-10-06T11:30:00.000-06:00'\n",
    "\n",
    "from dateutil import parser\n",
    "yourdate = parser.parse(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dea0dff6-16c2-40bd-83f1-999e5146860d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 10, 6, 11, 30, tzinfo=tzoffset(None, -21600))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yourdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d8844-e5db-47c0-956d-b872c4d1dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
